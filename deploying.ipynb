{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":42438,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":35657}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install emoji","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:58:10.479313Z","iopub.execute_input":"2024-05-08T01:58:10.479711Z","iopub.status.idle":"2024-05-08T01:58:26.923242Z","shell.execute_reply.started":"2024-05-08T01:58:10.479679Z","shell.execute_reply":"2024-05-08T01:58:26.922013Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (2.11.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:58:26.926007Z","iopub.execute_input":"2024-05-08T01:58:26.926430Z","iopub.status.idle":"2024-05-08T01:58:41.637044Z","shell.execute_reply.started":"2024-05-08T01:58:26.926394Z","shell.execute_reply":"2024-05-08T01:58:41.636035Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:58:41.639330Z","iopub.execute_input":"2024-05-08T01:58:41.639711Z","iopub.status.idle":"2024-05-08T01:58:58.793269Z","shell.execute_reply.started":"2024-05-08T01:58:41.639676Z","shell.execute_reply":"2024-05-08T01:58:58.791840Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install jieba","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:58:58.794789Z","iopub.execute_input":"2024-05-08T01:58:58.795174Z","iopub.status.idle":"2024-05-08T01:59:13.629893Z","shell.execute_reply.started":"2024-05-08T01:58:58.795141Z","shell.execute_reply":"2024-05-08T01:59:13.628315Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (0.42.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install indoNLP","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:13.633545Z","iopub.execute_input":"2024-05-08T01:59:13.633983Z","iopub.status.idle":"2024-05-08T01:59:29.163098Z","shell.execute_reply.started":"2024-05-08T01:59:13.633943Z","shell.execute_reply":"2024-05-08T01:59:29.161783Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting indoNLP\n  Downloading indoNLP-0.3.4-py3-none-any.whl.metadata (3.4 kB)\nDownloading indoNLP-0.3.4-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: indoNLP\nSuccessfully installed indoNLP-0.3.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install easygoogletranslate","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:29.164574Z","iopub.execute_input":"2024-05-08T01:59:29.164932Z","iopub.status.idle":"2024-05-08T01:59:44.375554Z","shell.execute_reply.started":"2024-05-08T01:59:29.164900Z","shell.execute_reply":"2024-05-08T01:59:44.373950Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting easygoogletranslate\n  Downloading easygoogletranslate-0.0.4-py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from easygoogletranslate) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->easygoogletranslate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->easygoogletranslate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->easygoogletranslate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->easygoogletranslate) (2024.2.2)\nDownloading easygoogletranslate-0.0.4-py3-none-any.whl (3.9 kB)\nInstalling collected packages: easygoogletranslate\nSuccessfully installed easygoogletranslate-0.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nimport jieba\nimport string\nimport emoji\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom indoNLP.preprocessing import replace_slang\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom snowballstemmer import stemmer\nfrom easygoogletranslate import EasyGoogleTranslate\n\nimport torch\nimport torch.nn as nn\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of BertForSequenceClassification were not initialized from the model checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:44.378300Z","iopub.execute_input":"2024-05-08T01:59:44.378676Z","iopub.status.idle":"2024-05-08T01:59:50.480354Z","shell.execute_reply.started":"2024-05-08T01:59:44.378642Z","shell.execute_reply":"2024-05-08T01:59:50.479111Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# abbreviation list taken from https://www.kaggle.com/code/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing#Slangs\n# English Abbreviation\nabbreviations = {\n    \"$\": \" dollar \",\n    \"€\": \" euro \",\n    \"4ao\": \"for adults only\",\n    \"a.m\": \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\",\n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\",\n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"pls\" : \"please\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\",\n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\",\n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.482698Z","iopub.execute_input":"2024-05-08T01:59:50.483360Z","iopub.status.idle":"2024-05-08T01:59:50.514371Z","shell.execute_reply.started":"2024-05-08T01:59:50.483325Z","shell.execute_reply":"2024-05-08T01:59:50.513380Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class EnTextCleaner:\n    def __init__(self, text, abbreviations):\n        self.dataframe = pd.DataFrame(data={'text':[text]})\n        self.abbreviations = abbreviations\n\n    def lower_casing(self):\n        \"\"\"Convert all text to lowercase.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe[\"text\"].str.lower()\n        return self\n\n    def remove_urls(self):\n        \"\"\"Remove urls.\"\"\"\n        url_re = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe[\"cleaned_text\"].apply(lambda text: url_re.sub('', text))\n        return self\n\n    def remove_usernames(self):\n        \"\"\"Remove usernames that start with '@'.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: re.sub('@[^\\s]+', '', text))\n        return self\n\n    def remove_html(self):\n        \"\"\"Remove HTML tags.\"\"\"\n        tag_re = re.compile(r'<[^>]+>')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: tag_re.sub('', text))\n        return self\n\n    def remove_hex_and_unicode(self):  #found in indonesian\n        \"\"\"Remove hexadecimal and Unicode escape sequences.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(\n            lambda text: re.sub(r'\\\\x[0-9a-fA-F]+|\\\\u[0-9a-fA-F]{4}', '', text))\n        return self\n\n    def remove_emoji(self):\n        \"\"\"Remove emoji from text\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: emoji.replace_emoji(text))\n        return self\n\n    def remove_punctuation(self):\n        \"\"\"Remove punctuation from the text, including Chinese punctuation if specified.\"\"\"\n        en_punct_pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: en_punct_pattern.sub('', text))\n        return self\n\n    def remove_numbers(self):\n        \"\"\"Remove numbers\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: re.sub('\\d+', '', text))\n        return self\n\n    def remove_whitespace(self):\n        \"\"\"Remove whitespaces\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: \" \".join(text.split()))\n        return self\n\n    def chat_word_conversion(self):\n        \"\"\"Convert chat words into formal words\"\"\"\n        def convert_abbrev(text):\n            #tokenize the text first\n            tokens = word_tokenize(text)\n            converted_tokens = [self.abbreviations.get(token.lower(), token) for token in tokens]\n            return ' ' .join(converted_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(convert_abbrev)\n        return self\n\n    def remove_en_stopwords(self):\n        \"\"\"Remove stopwords\"\"\"\n        def remove_stopwords(text):\n            tokens = word_tokenize(text)\n            english_stopwords = set(stopwords.words('english')) | {\"retweet\", \"user\"}\n            removed_tokens = [token for token in tokens if token not in english_stopwords]\n            return ' ' .join(removed_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(remove_stopwords)\n        return self\n\n    def en_lemmatizer(self):\n        \"\"\"Lemmatize English\"\"\"\n\n        def lemmatize_words(text):\n            \"\"\"Function to lemmatize words in a list of tokens\"\"\"\n            lemmatizer = WordNetLemmatizer()\n            wordnet_map = {\n                'N': wordnet.NOUN,\n                'V': wordnet.VERB,\n                'R': wordnet.ADV,\n                'J': wordnet.ADJ\n            }\n\n            tokens = word_tokenize(text)\n\n            # Perform part-of-speech tagging\n            pos_tagged_tokens = nltk.pos_tag(tokens)\n            # Lemmatize each word based on its part-of-speech\n            lemmatized_tokens = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_tokens]\n            return ' ' .join(lemmatized_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lemmatize_words)\n        return self\n\n    def get_cleaned_dataframe(self):\n        \"\"\"Return the cleaned DataFrame.\"\"\"\n        return self.dataframe","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.516129Z","iopub.execute_input":"2024-05-08T01:59:50.516929Z","iopub.status.idle":"2024-05-08T01:59:50.544508Z","shell.execute_reply.started":"2024-05-08T01:59:50.516886Z","shell.execute_reply":"2024-05-08T01:59:50.542956Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# take from online and combine chatgpt generated stopwords\n# https://www.ranks.nl/stopwords/chinese-stopwords\nzh_stopwords = ['的','一','不','在','人','有','是','为','以','于','上','他','而','后','之','来','及','了',\n                '因','下','可','到','由','这','与','也','此','但','并','个','其','已','无','小','我','们',\n                '起','最','再','今','去','好','只','又','或','很','亦','某','把','那','你','乃','它','吧',\n                '被','比','别','趁','当','从','得','打','凡','儿','尔','该','各','给','跟','和','何','还',\n                '即','几','既','看','据','距','靠','啦','另','么','每','嘛','拿','哪','您','凭','且','却',\n                '让','仍','啥','如','若','使','谁','虽','随','同','所','她','哇','嗡','往','些','向','沿',\n                '哟','用','咱','则','怎','曾','至','致','着','诸','自','呢','吗','啊','哦','呀','噢','哈',\n                '呃','嗯','唉','啦','呗','嘿','哒','啵','喽','嘛','咧','咋','哩','呼','吱','呜','咦','咚',\n                '哐','咔','哧','咕','呶','噻','哇','哔','嗖','嘎','嗒','嘘','嘁','呕','啪','啷','啪','喳',\n                '嘤','轰','哼','唿','嚯','呵','嚓','哒','嗡','嘻','嘟','嗑','嗬','嗔','嗦','嗝','嗄','嗯', \n                '嗨','喽','嘿','呀','呦','矣','哉','俺','尔','耶','呗','咻','咿','哎','哏','哗','咯','啰', \n                '啧','喏','喔','嗷','嘈','嘤','嗉','呷','呱','呤','噼','啪','叽','咣','咭','哒','嗒','嗤',\n                '哙','哚','哜','嗖','嗑','嗲','嘚','嗌','嘧','嘭','哓','嗵','都']","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.546239Z","iopub.execute_input":"2024-05-08T01:59:50.546594Z","iopub.status.idle":"2024-05-08T01:59:50.565436Z","shell.execute_reply.started":"2024-05-08T01:59:50.546567Z","shell.execute_reply":"2024-05-08T01:59:50.563980Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ZhTextCleaner:\n    def __init__(self, text, zh_stopwords):\n        self.dataframe = pd.DataFrame(data={'text':[text]})\n        self.zh_stopwords = zh_stopwords\n\n    def remove_urls(self):\n        \"\"\"Remove urls.\"\"\"\n        url_re = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe[\"text\"].apply(lambda text: url_re.sub('', text))\n        return self\n\n    def remove_usernames(self):\n        \"\"\"Remove usernames that start with '@'.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: re.sub('@[^\\s]+', '', text))\n        return self\n\n    def remove_html(self):\n        \"\"\"Remove HTML tags.\"\"\"\n        tag_re = re.compile(r'<[^>]+>')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: tag_re.sub('', text))\n        return self\n\n    def remove_hex_and_unicode(self):  #found in indonesian\n        \"\"\"Remove hexadecimal and Unicode escape sequences.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(\n            lambda text: re.sub(r'\\\\x[0-9a-fA-F]+|\\\\u[0-9a-fA-F]{4}', '', text))\n        return self\n\n    def remove_emoji(self):\n        \"\"\"Remove emoji from text\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: emoji.replace_emoji(text))\n        return self\n\n    def remove_punctuation(self):\n        \"\"\"Remove punctuation from the text, including Chinese punctuation if specified.\"\"\"\n        zh_punct_pattern = re.compile('[' + re.escape(string.punctuation) + \"\\u3000-\\u303F\" +\"\\uFF00-\\uFFEF\" +\"\\u2000-\\u206F\" +\"\\u2E00-\\u2E7F\" + ']+')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: zh_punct_pattern.sub('', text))\n        return self\n\n    def remove_numbers(self):\n        \"\"\"Remove numbers\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: re.sub('\\d+', '', text))\n        return self\n\n    def remove_whitespace(self):\n        \"\"\"Remove whitespaces\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: \" \".join(text.split()))\n        return self\n\n    def remove_zh_stopwords(self):\n        \"\"\"Remove stopwords\"\"\"\n        def remove_stopwords(text):\n            tokens = jieba.lcut(text)\n            removed_tokens = [token for token in tokens if token not in self.zh_stopwords]\n            return ' ' .join(removed_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(remove_stopwords)\n        return self\n\n    def get_cleaned_dataframe(self):\n        \"\"\"Return the cleaned DataFrame.\"\"\"\n        return self.dataframe","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.568872Z","iopub.execute_input":"2024-05-08T01:59:50.569308Z","iopub.status.idle":"2024-05-08T01:59:50.589853Z","shell.execute_reply.started":"2024-05-08T01:59:50.569267Z","shell.execute_reply":"2024-05-08T01:59:50.588769Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class IdTextCleaner:\n    def __init__(self, text, abbreviations):\n        self.dataframe = pd.DataFrame(data={'text':[text]})\n        self.abbreviations = abbreviations\n\n    def lower_casing(self):\n        \"\"\"Convert all text to lowercase.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe[\"text\"].str.lower()\n        return self\n\n    def remove_urls(self):\n        \"\"\"Remove urls.\"\"\"\n        url_re = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe[\"cleaned_text\"].apply(lambda text: url_re.sub('', text))\n        return self\n\n    def remove_usernames(self):\n        \"\"\"Remove usernames that start with '@'.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: re.sub('@[^\\s]+', '', text))\n        return self\n\n    def remove_html(self):\n        \"\"\"Remove HTML tags.\"\"\"\n        tag_re = re.compile(r'<[^>]+>')\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: tag_re.sub('', text))\n        return self\n\n    def remove_hex_and_unicode(self):  #found in indonesian\n        \"\"\"Remove hexadecimal and Unicode escape sequences.\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(\n            lambda text: re.sub(r'\\\\x[0-9a-fA-F]+|\\\\u[0-9a-fA-F]{4}|[\\u2600-\\u26FF]', '', text))\n        return self\n\n    def remove_emoji(self):\n        \"\"\"Remove emoji from text\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: emoji.replace_emoji(text))\n        return self\n\n    def remove_punctuation(self):\n        \"\"\"Remove punctuation from the text, including Chinese punctuation if specified.\"\"\"\n        en_punct_pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: en_punct_pattern.sub('', text))\n        return self\n\n    def remove_numbers(self):\n        \"\"\"Remove numbers\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: re.sub('\\d+', '', text))\n        return self\n\n    def remove_whitespace(self):\n        \"\"\"Remove whitespaces\"\"\"\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(lambda text: \" \".join(text.split()))\n        return self\n\n    def chat_word_conversion(self):\n        \"\"\"Convert chat words into formal words\"\"\"\n        def convert_abbrev(text):\n            tokens = word_tokenize(text)\n            converted_tokens = [self.abbreviations.get(token.lower(), token) for token in tokens]\n            return ' ' .join(converted_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(convert_abbrev)\n        return self\n\n    def replace_slang(self):\n        \"\"\"Convert slang into formal words\"\"\"\n        def replace_indonesian_slang(text):\n            text_with_slang_replaced = replace_slang(text)\n            return text_with_slang_replaced\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(replace_indonesian_slang)\n        return self\n\n    def remove_id_stopwords(self):\n        \"\"\"Remove stopwords\"\"\"\n        def remove_stopwords(text):\n            tokens = word_tokenize(text)\n            indonesian_stopwords = set(stopwords.words('indonesian')) | {\"retweet\", \"user\"}\n            removed_tokens = [token for token in tokens if token not in indonesian_stopwords]\n            return ' ' .join(removed_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(remove_stopwords)\n        return self\n\n    def id_stemmer(self):\n        \"\"\"Stem indonesian text\"\"\"\n        def stem_indonesian(text):\n            tokens = word_tokenize(text)\n            stemmer_id = stemmer(\"indonesian\")\n            stemmed_tokens = [stemmer_id.stemWord(token) for token in tokens]\n            return ' ' .join(stemmed_tokens)\n\n        self.dataframe.loc[:, 'cleaned_text'] = self.dataframe['cleaned_text'].apply(stem_indonesian)\n        return self\n\n    def get_cleaned_dataframe(self):\n        \"\"\"Return the cleaned DataFrame.\"\"\"\n        return self.dataframe","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.591679Z","iopub.execute_input":"2024-05-08T01:59:50.592140Z","iopub.status.idle":"2024-05-08T01:59:50.621490Z","shell.execute_reply.started":"2024-05-08T01:59:50.592108Z","shell.execute_reply":"2024-05-08T01:59:50.620060Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def preprocessing(text, language, abbreviations, zh_stopwords):\n    if language=='en':\n        #peform preprocessing for each language\n        en_cleaner = EnTextCleaner(text, abbreviations)\n        en_cleaner.lower_casing()\n        en_cleaner.remove_urls()\n        en_cleaner.remove_usernames()\n        en_cleaner.remove_html()\n        en_cleaner.remove_hex_and_unicode()\n        en_cleaner.remove_emoji()\n        en_cleaner.remove_punctuation()\n        en_cleaner.remove_numbers()\n        en_cleaner.remove_whitespace()\n        en_cleaner.chat_word_conversion()\n        en_cleaner.remove_en_stopwords()\n        en_cleaner.en_lemmatizer()\n        en_cleaned = en_cleaner.get_cleaned_dataframe()\n        return en_cleaned.iloc[0]['cleaned_text']\n    \n    elif language=='zh':\n        zh_cleaner = ZhTextCleaner(text, zh_stopwords)\n        zh_cleaner.remove_urls()\n        zh_cleaner.remove_usernames()\n        zh_cleaner.remove_html()\n        zh_cleaner.remove_hex_and_unicode()\n        zh_cleaner.remove_emoji()\n        zh_cleaner.remove_punctuation()\n        zh_cleaner.remove_numbers()\n        zh_cleaner.remove_whitespace()\n        zh_cleaner.remove_zh_stopwords()\n        zh_cleaned = zh_cleaner.get_cleaned_dataframe()\n        return zh_cleaned.iloc[0]['cleaned_text']\n    \n    elif language=='id':\n        id_cleaner = IdTextCleaner(text, abbreviations)\n        id_cleaner.lower_casing()\n        id_cleaner.remove_urls()\n        id_cleaner.remove_usernames()\n        id_cleaner.remove_html()\n        id_cleaner.remove_hex_and_unicode()\n        id_cleaner.remove_emoji()\n        id_cleaner.remove_punctuation()\n        id_cleaner.remove_numbers()\n        id_cleaner.remove_whitespace()\n        id_cleaner.chat_word_conversion()\n        id_cleaner.replace_slang()\n        id_cleaner.remove_id_stopwords()\n        id_cleaner.id_stemmer()\n        id_cleaned = id_cleaner.get_cleaned_dataframe()\n        return id_cleaned.iloc[0]['cleaned_text']\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.623377Z","iopub.execute_input":"2024-05-08T01:59:50.624278Z","iopub.status.idle":"2024-05-08T01:59:50.640154Z","shell.execute_reply.started":"2024-05-08T01:59:50.624236Z","shell.execute_reply":"2024-05-08T01:59:50.639061Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# class SimpleBiLSTM(nn.Module):\n#     def __init__(self, embedding_dim, hidden_dim,num_layers,batch_first,bidirectional):\n#         super(SimpleBiLSTM, self).__init__()\n#         # Define a bidirectional LSTM\n#         self.lstm = nn.LSTM(input_size=embedding_dim, \n#                             hidden_size=hidden_dim, \n#                             num_layers=num_layers,\n#                             batch_first=batch_first, \n#                             bidirectional=bidirectional)\n        \n#         # Define a fully connected layer to map the concatenated hidden states to a single output\n#         self.fc = nn.Linear(in_features=2 * hidden_dim, out_features=1)\n\n#     def forward(self, x):\n#         # Forward pass through LSTM layer\n#         _, (hn, _) = self.lstm(x)  # hn contains the hidden state for each direction of each layer\n        \n#         # Concatenate the last hidden states of the forward and backward passes from the last layer\n#         hn_cat = torch.cat((hn[-2], hn[-1]), dim=1)\n\n#         # Pass the concatenated hidden states through the fully connected layer and apply sigmoid activation\n#         out = self.fc(hn_cat)\n#         out = torch.sigmoid(out)\n\n#         # Squeeze the output to remove the singleton dimension to match expected output shape (batch_size,)\n#         return out.squeeze(1)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.644774Z","iopub.execute_input":"2024-05-08T01:59:50.645599Z","iopub.status.idle":"2024-05-08T01:59:50.658336Z","shell.execute_reply.started":"2024-05-08T01:59:50.645562Z","shell.execute_reply":"2024-05-08T01:59:50.657112Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.659807Z","iopub.execute_input":"2024-05-08T01:59:50.661178Z","iopub.status.idle":"2024-05-08T01:59:50.680058Z","shell.execute_reply.started":"2024-05-08T01:59:50.661140Z","shell.execute_reply":"2024-05-08T01:59:50.678924Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"/kaggle/input/mbert-models/pytorch/mbert/1/mBERT_over.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmBERT_model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=2)\nmBERT_model.load_state_dict(torch.load('/kaggle/input/mbert-models/pytorch/mbert/1/mBERT_over.pth', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T01:59:50.681535Z","iopub.execute_input":"2024-05-08T01:59:50.682216Z","iopub.status.idle":"2024-05-08T02:00:06.936271Z","shell.execute_reply.started":"2024-05-08T01:59:50.682185Z","shell.execute_reply":"2024-05-08T02:00:06.935066Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee889c936ade40bf96c9c8116d710b82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9a4ee6a9b864b69a22b236f21c4054b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8df02bd235c469fb7efea5557b5d7f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e60174a29ef0463498b39be215bc62d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69aa6f2647d0456aace1245b220bcc99"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def predict_class_mBert(input_text, language, abbreviations, zh_stopwords):\n    # Assuming preprocessing is defined elsewhere\n    input_data = preprocessing(input_text, language, abbreviations, zh_stopwords)\n    input_ids = []\n    attention_masks = []\n\n    # Tokenize all texts\n    encoded = tokenizer.encode_plus(\n        input_data,\n        add_special_tokens=True,\n        max_length=256,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True\n    )\n    input_ids.append(encoded['input_ids'])\n    attention_masks.append(encoded['attention_mask'])\n\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    with torch.no_grad():\n        outputs = mBERT_model(input_ids, attention_mask=attention_masks)\n    \n    logits = outputs.logits\n    probs = torch.softmax(logits, dim=1)\n    predictions = torch.argmax(logits, dim=1)\n\n    results = []\n    for i, prob in enumerate(probs):\n        pred_label = predictions[i].item()\n        prob_of_pred_class = prob[pred_label].item()\n        classification = \"Non-Hate Speech\" if pred_label == 1 else \"Hate Speech\"\n        results.append((prob_of_pred_class, classification))\n    \n    return results[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:06.938074Z","iopub.execute_input":"2024-05-08T02:00:06.938636Z","iopub.status.idle":"2024-05-08T02:00:06.951572Z","shell.execute_reply.started":"2024-05-08T02:00:06.938602Z","shell.execute_reply":"2024-05-08T02:00:06.949783Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display\ntext = \"I hate you\"\nresult = predict_class_mBert(text, \"en\", abbreviations, zh_stopwords)\n\nprint(\"Analysing...\", text)\nprint(f\"Probability of being '{result[1]}' is {result[0] * 100:.2f}%\")\nprint(\"Final Verdict:\", result[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:06.952964Z","iopub.execute_input":"2024-05-08T02:00:06.953405Z","iopub.status.idle":"2024-05-08T02:00:10.382328Z","shell.execute_reply.started":"2024-05-08T02:00:06.953373Z","shell.execute_reply":"2024-05-08T02:00:10.381169Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Analysing... I hate you\nProbability of being 'Hate Speech' is 79.10%\nFinal Verdict: Hate Speech\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display\ntext = \"I love you so much!!\"\nresult = predict_class_mBert(text, \"en\", abbreviations, zh_stopwords)\n\nprint(\"Analysing...\", text)\nprint(f\"Probability of being '{result[1]}' is {result[0] * 100:.2f}%\")\nprint(\"Final Verdict:\", result[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:10.384190Z","iopub.execute_input":"2024-05-08T02:00:10.384747Z","iopub.status.idle":"2024-05-08T02:00:10.880973Z","shell.execute_reply.started":"2024-05-08T02:00:10.384708Z","shell.execute_reply":"2024-05-08T02:00:10.879531Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Analysing... I love you so much!!\nProbability of being 'Non-Hate Speech' is 58.37%\nFinal Verdict: Non-Hate Speech\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display\ntext = \"你去死吧\"\nresult = predict_class_mBert(text, \"zh\", abbreviations, zh_stopwords)\n\nprint(\"Analysing...\", text)\nprint(f\"Probability of being '{result[1]}' is {result[0] * 100:.2f}%\")\nprint(\"Final Verdict:\", result[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:10.882517Z","iopub.execute_input":"2024-05-08T02:00:10.882906Z","iopub.status.idle":"2024-05-08T02:00:12.759648Z","shell.execute_reply.started":"2024-05-08T02:00:10.882872Z","shell.execute_reply":"2024-05-08T02:00:12.758374Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 1.381 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"name":"stdout","text":"Analysing... 你去死吧\nProbability of being 'Hate Speech' is 82.00%\nFinal Verdict: Hate Speech\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display\ntext = \"你爱我呀\"\nresult = predict_class_mBert(text, \"zh\", abbreviations, zh_stopwords)\n\nprint(\"Analysing...\", text)\nprint(f\"Probability of being '{result[1]}' is {result[0] * 100:.2f}%\")\nprint(\"Final Verdict:\", result[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:12.761378Z","iopub.execute_input":"2024-05-08T02:00:12.761765Z","iopub.status.idle":"2024-05-08T02:00:13.245336Z","shell.execute_reply.started":"2024-05-08T02:00:12.761733Z","shell.execute_reply":"2024-05-08T02:00:13.243983Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Analysing... 你爱我呀\nProbability of being 'Non-Hate Speech' is 61.85%\nFinal Verdict: Non-Hate Speech\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display\ntext = \"Kamu pergi mati\"\nresult = predict_class_mBert(text, \"id\", abbreviations, zh_stopwords)\n\nprint(\"Analysing...\", text)\nprint(f\"Probability of being '{result[1]}' is {result[0] * 100:.2f}%\")\nprint(\"Final Verdict:\", result[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:13.246842Z","iopub.execute_input":"2024-05-08T02:00:13.247239Z","iopub.status.idle":"2024-05-08T02:00:13.911302Z","shell.execute_reply.started":"2024-05-08T02:00:13.247207Z","shell.execute_reply":"2024-05-08T02:00:13.910353Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Analysing... Kamu pergi mati\nProbability of being 'Hate Speech' is 63.51%\nFinal Verdict: Hate Speech\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display\ntext = \"Sekarang saya tidur\"\nresult = predict_class_mBert(text, \"id\", abbreviations, zh_stopwords)\n\nprint(\"Analysing...\", text)\nprint(f\"Probability of being '{result[1]}' is {result[0] * 100:.2f}%\")\nprint(\"Final Verdict:\", result[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T02:00:13.912687Z","iopub.execute_input":"2024-05-08T02:00:13.913855Z","iopub.status.idle":"2024-05-08T02:00:14.413045Z","shell.execute_reply.started":"2024-05-08T02:00:13.913810Z","shell.execute_reply":"2024-05-08T02:00:14.411741Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Analysing... Sekarang saya tidur\nProbability of being 'Non-Hate Speech' is 88.87%\nFinal Verdict: Non-Hate Speech\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}